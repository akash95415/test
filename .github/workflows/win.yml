name: Ollama Llama3 8B Eval

on:
  workflow_dispatch:
    inputs:
      prompt:
        description: "Prompt to send to Ollama"
        required: true
        default: "Benchmark: generate 128 tokens of simple text to measure speed."

jobs:
  run-ollama:
    runs-on: ubuntu-latest  # strongest free runner (4 vCPU, 14 GB RAM)

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y curl jq

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          ollama --version

      - name: Start Ollama server
        run: |
          nohup ollama serve > /tmp/ollama.log 2>&1 &
          sleep 8
          curl -s http://localhost:11434 || (echo "Ollama server did not start" && exit 1)

      - name: Pull llama3:8b
        run: ollama pull llama3:8b

      - name: Run prompt in streaming mode
        run: |
          echo "Prompt: ${{ github.event.inputs.prompt }}"
          curl -N -s -X POST http://localhost:11434/api/generate \
            -H "Content-Type: application/json" \
            -d "{\"model\":\"llama3:8b\",\"prompt\":\"${{ github.event.inputs.prompt}}\",\"stream\":true}"
